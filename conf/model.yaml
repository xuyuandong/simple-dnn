### Model Parameter Configuration

## Linear Parameters

# linear_optimizer:
#     Required, one of {`Adagrad`, `Adam`, `Ftrl`, `RMSProp`, `SGD`} or
#     use tf.train.Optimizer instance to pass specific optimizer args.
# linear_initial_learning_rate:
#     Optional, initial value of lr, if not specified, defaults to 0.05, can be override by tf.train.Optimizer instance lr args.
# linear_decay_rate:
#     Optional, decay rate for each epoch, if not specified, defaults to 1, set empty or 1 to not use weight decay.
#     decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)
#     After a long time of keep training, set proper small learning rate and turn off weight decay.
linear_optimizer: tf.train.FtrlOptimizer(learning_rate=0.1,l1_regularization_strength=0.001,l2_regularization_strength=0.05)
linear_initial_learning_rate: 0.05
linear_decay_rate: 0.8


## DNN Parameters

# dnn_hidden_units: A list indicate each hidden units number.
#     set nested hidden_units, eg: [[1024,512,256], [512,256]] for multi dnn model.
# dnn_connected_mode:
#    One of {`simple`, `first_dense`, `last_dense`, `dense`, `resnet`} or arbitrary connections.
#      1. `simple`: normal dnn architecture.
#      2. `first_dense`: add connections from first input layer to all hidden layers.
#      3. `last_dense`: add connections from all previous layers to last layer.
#      4. `dense`: add connections between all layers, similar to DenseNet.
#      5. `resnet`: add connections between adjacent layers, similar to ResNet.
#      6. arbitrary connections list: add connections from layer_0 to layer_1 like 0-1.
#        eg: [0-1,0-3,1-2]  index start from zero(input_layer), max index is len(hidden_units), smaller index first.
#    Set different for each dnn eg: ['simple', 'dense'] or use same mode if only set 'simple'

# dnn_optimizer:
# dnn_initial_learning_rate: if not specified, defaults to 0.05.
# dnn_decay_rate:
#     above 3 paramters see linear, use same for multidnn.
# dnn_activation_function:
#     One of {`sigmoid`,`tanh`,`relu`,`relu6`,`leaky_relu`,`crelu`,`elu`,`selu`,`softplus`,`softsign`}
# dnn_l1: L1 regularization for dense layers, set 0 or empty to not use.
# dnn_l2: L2 regularization for dense layers, set 0 or empty to not use.
# dnn_dropout: dropout rate, 0.1 would drop out 10% of input units, set 0 or empty to not use.
# dnn_batch_normalization: Bool, set 1 or True to enable do batch normalization.

dnn_hidden_units: [1024,512,256]
dnn_connected_mode: simple
dnn_optimizer: Adagrad
dnn_initial_learning_rate: 0.05
dnn_decay_rate: 0.8
dnn_activation_function: relu
dnn_l1: 0.00001
dnn_l2: 0.0001
dnn_dropout:
dnn_batch_normalization: 1




